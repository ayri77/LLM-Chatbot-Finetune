# LLM-Chatbot-Finetune

This project focuses on fine-tuning a pre-trained language model (LLM) to create a more personalized and domain-adapted chatbot. The goal is to explore fine-tuning techniques (LoRA, QLoRA) using real-world conversational data from previous chatbot applications and educational materials.

## Goals
- Learn and implement LLM fine-tuning workflows
- Understand the transformer architecture through hands-on experiments
- Build a customizable chatbot capable of adapting to new domains
- Test various models (Mistral, Phi-2, etc.) and techniques (instruction tuning, conversational fine-tuning)

## Tools & Stack
- PyTorch, Hugging Face Transformers
- LoRA / QLoRA for efficient fine-tuning
- Weights & Biases (optional) for experiment tracking
- Streamlit or Gradio for frontend UI

## Current Status
‚úÖ Project structure initialized  
‚è≥ Preparing dataset and scripts for fine-tuning  
üîú Model evaluation and chatbot deployment  
