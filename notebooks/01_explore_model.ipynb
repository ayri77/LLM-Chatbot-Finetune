{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "import torch\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.utils import login_huggingface, load_model\n",
    "from utils.main_train_model import evaluate_model, summarize_all_results\n",
    "from config import RESULTS_DIR\n",
    "\n",
    "login_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356adcc386844f36a9ee2220400fcfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐšÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹ ÐÐ»ÑŒÐ±ÐµÑ€Ñ‚ Ð­Ð¹Ð½ÑˆÑ‚ÐµÐ¹Ð½?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸, ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÑƒÑÐº Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ ÑÐºÐ¾Ð»Ð¾Ð³Ð¸Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸: 'I am testing a language model' Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ð¾Ðµ Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸, Ð³Ð´Ðµ Ð¾Ð½Ð¾ Ð¶Ð¸Ð²Ñ‘Ñ‚\n",
      "A:\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: results\\mistral.json, results\\mistral.csv\n",
      "ðŸ”„ deepseek_chat: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23680c14a8864ef5afa5d1a84c8beef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_chat: Q: ÐšÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹ ÐÐ»ÑŒÐ±ÐµÑ€Ñ‚ Ð­Ð¹Ð½ÑˆÑ‚ÐµÐ¹Ð½?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_chat: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸, ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÑƒÑÐº Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_chat: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ ÑÐºÐ¾Ð»Ð¾Ð³Ð¸Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_chat: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸: 'I am testing a language model' Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_chat: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ð¾Ðµ Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸, Ð³Ð´Ðµ Ð¾Ð½Ð¾ Ð¶Ð¸Ð²Ñ‘Ñ‚\n",
      "A:\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: results\\deepseek_chat.json, results\\deepseek_chat.csv\n",
      "ðŸ”„ deepseek_base: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90999c729ea846e58c16b640113a3f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_base: Q: ÐšÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹ ÐÐ»ÑŒÐ±ÐµÑ€Ñ‚ Ð­Ð¹Ð½ÑˆÑ‚ÐµÐ¹Ð½?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_base: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸, ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÑƒÑÐº Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_base: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ ÑÐºÐ¾Ð»Ð¾Ð³Ð¸Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_base: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸: 'I am testing a language model' Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_base: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ð¾Ðµ Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸, Ð³Ð´Ðµ Ð¾Ð½Ð¾ Ð¶Ð¸Ð²Ñ‘Ñ‚\n",
      "A:\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: results\\deepseek_base.json, results\\deepseek_base.csv\n",
      "ðŸ”„ zephyr: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fc8048b40146219442e25557f5482d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ zephyr: Q: ÐšÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹ ÐÐ»ÑŒÐ±ÐµÑ€Ñ‚ Ð­Ð¹Ð½ÑˆÑ‚ÐµÐ¹Ð½?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ zephyr: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸, ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÑƒÑÐº Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ zephyr: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ ÑÐºÐ¾Ð»Ð¾Ð³Ð¸Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ zephyr: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸: 'I am testing a language model' Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ zephyr: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ð¾Ðµ Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸, Ð³Ð´Ðµ Ð¾Ð½Ð¾ Ð¶Ð¸Ð²Ñ‘Ñ‚\n",
      "A:\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: results\\zephyr.json, results\\zephyr.csv\n",
      "ðŸ”„ Meta Llama: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a47ce85977f488baa8d4685de978a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐšÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹ ÐÐ»ÑŒÐ±ÐµÑ€Ñ‚ Ð­Ð¹Ð½ÑˆÑ‚ÐµÐ¹Ð½?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸, ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÑƒÑÐº Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ ÑÐºÐ¾Ð»Ð¾Ð³Ð¸Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸: 'I am testing a language model' Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ð¾Ðµ Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸, Ð³Ð´Ðµ Ð¾Ð½Ð¾ Ð¶Ð¸Ð²Ñ‘Ñ‚\n",
      "A:\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: results\\Meta Llama.json, results\\Meta Llama.csv\n",
      "ðŸ”„ MS phi-2: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee3b7e97a9e41bc8f54cccebb6bebe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ MS phi-2: Q: ÐšÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹ ÐÐ»ÑŒÐ±ÐµÑ€Ñ‚ Ð­Ð¹Ð½ÑˆÑ‚ÐµÐ¹Ð½?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ MS phi-2: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸, ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÑƒÑÐº Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ MS phi-2: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ ÑÐºÐ¾Ð»Ð¾Ð³Ð¸Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ MS phi-2: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸: 'I am testing a language model' Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ MS phi-2: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ð¾Ðµ Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸, Ð³Ð´Ðµ Ð¾Ð½Ð¾ Ð¶Ð¸Ð²Ñ‘Ñ‚\n",
      "A:\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: results\\MS phi-2.json, results\\MS phi-2.csv\n",
      "ðŸ”„ deepseek_r1_qwen8b: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11cae1415a6433bb064c8b46ab49ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐšÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹ ÐÐ»ÑŒÐ±ÐµÑ€Ñ‚ Ð­Ð¹Ð½ÑˆÑ‚ÐµÐ¹Ð½?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸, ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÑƒÑÐº Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ ÑÐºÐ¾Ð»Ð¾Ð³Ð¸Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸: 'I am testing a language model' Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ð¾Ðµ Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸, Ð³Ð´Ðµ Ð¾Ð½Ð¾ Ð¶Ð¸Ð²Ñ‘Ñ‚\n",
      "A:\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: results\\deepseek_r1_qwen8b.json, results\\deepseek_r1_qwen8b.csv\n",
      "ðŸ”„ deepseek_r1_llama8b: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efafbad5a2648f496e47c75eb36252e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_llama8b: Q: ÐšÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹ ÐÐ»ÑŒÐ±ÐµÑ€Ñ‚ Ð­Ð¹Ð½ÑˆÑ‚ÐµÐ¹Ð½?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_llama8b: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸, ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÑƒÑÐº Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_llama8b: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ ÑÐºÐ¾Ð»Ð¾Ð³Ð¸Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_llama8b: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸: 'I am testing a language model' Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_llama8b: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ð¾Ðµ Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸, Ð³Ð´Ðµ Ð¾Ð½Ð¾ Ð¶Ð¸Ð²Ñ‘Ñ‚\n",
      "A:\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: results\\deepseek_r1_llama8b.json, results\\deepseek_r1_llama8b.csv\n",
      "ðŸ”„ Google gemma_7b: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52313f9170a489faa7d674648fe8d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Google gemma_7b: Q: ÐšÑ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ð¹ ÐÐ»ÑŒÐ±ÐµÑ€Ñ‚ Ð­Ð¹Ð½ÑˆÑ‚ÐµÐ¹Ð½?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Google gemma_7b: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸, ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿ÑƒÑÐº Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Google gemma_7b: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³ Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ ÑÐºÐ¾Ð»Ð¾Ð³Ð¸Ð¸\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Google gemma_7b: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸: 'I am testing a language model' Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Google gemma_7b: Q: ÐŸÑ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ð¾Ðµ Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸, Ð³Ð´Ðµ Ð¾Ð½Ð¾ Ð¶Ð¸Ð²Ñ‘Ñ‚\n",
      "A:\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: results\\Google gemma_7b.json, results\\Google gemma_7b.csv\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"deepseek_chat\":  \"deepseek-ai/deepseek-llm-7b-chat\",\n",
    "    \"deepseek_base\":  \"deepseek-ai/deepseek-llm-7b-base\",\n",
    "    \"zephyr\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"Meta Llama\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"MS phi-2\": \"microsoft/phi-2\",\n",
    "    \"deepseek_r1_qwen8b\":  \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "    \"deepseek_r1_llama8b\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"Google gemma_7b\": \"google/gemma-7b\"    \n",
    "}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    mdl = tok = None\n",
    "    print(f\"ðŸ”„ {name}: loadingâ€¦\")\n",
    "    try:\n",
    "        tok, mdl = load_model(model_id)          # load model\n",
    "        evaluate_model(mdl, tok, name)           # evaluate model\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {name}: {e}\")\n",
    "    finally:\n",
    "        del mdl, tok                             # unload model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_time_sec</th>\n",
       "      <th>prompt_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MS phi-2</td>\n",
       "      <td>3.246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek_chat</td>\n",
       "      <td>4.640</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mistral</td>\n",
       "      <td>4.920</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deepseek_base</td>\n",
       "      <td>5.260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meta Llama</td>\n",
       "      <td>5.720</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google gemma_7b</td>\n",
       "      <td>5.940</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek_r1_llama8b</td>\n",
       "      <td>6.478</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>zephyr</td>\n",
       "      <td>6.642</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deepseek_r1_qwen8b</td>\n",
       "      <td>8.062</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  avg_time_sec  prompt_count\n",
       "1             MS phi-2         3.246             5\n",
       "4        deepseek_chat         4.640             5\n",
       "7              mistral         4.920             5\n",
       "3        deepseek_base         5.260             5\n",
       "2           Meta Llama         5.720             5\n",
       "0      Google gemma_7b         5.940             5\n",
       "5  deepseek_r1_llama8b         6.478             5\n",
       "8               zephyr         6.642             5\n",
       "6   deepseek_r1_qwen8b         8.062             5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore models\n",
    "summary_df = summarize_all_results()\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d59983bd35341de94d00670e3fc4c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Perfekt Ð³Ð»Ð°Ð³Ð¾Ð»Ð¾Ð² Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð½Ð° \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐšÐ¾Ð³Ð´Ð° Ð² Ð½ÐµÐ¼ÐµÑ†ÐºÐ¾Ð¼ ÑƒÐ¿Ð¾Ñ‚Ñ€ÐµÐ±Ð»ÑÐµÑ‚ÑÑ Konjunktiv II? ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ð¾-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¸Ð¹ Ð¸ Ð¿Ñ€Ð¾ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹ Ð³Ñ€Ð°Ð¼Ð¼Ð°Ñ‚Ð¸ÐºÑƒ: â€žIch hab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸ Ð½Ð° Ð½ÐµÐ¼ÐµÑ†ÐºÐ¸Ð¹: Â«Ð¯ Ñ…Ð¾Ñ‚ÐµÐ» Ð±Ñ‹ Ð·Ð°ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ñ‡Ð°ÑˆÐºÑƒ ÐºÐ¾Ñ„Ðµ, Ð¿Ð¾\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚ Ð½Ð°Ð¿Ð¸ÑÐ°Ð»: â€žIch gehe am Montag ins Schule.â€œ ÐÐ°Ð¹Ð´Ð¸ Ð¾\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚: â€žEr hat ein Auto gekauft gestern.â€œ Ð˜ÑÐ¿Ñ€Ð°Ð²ÑŒ Ð¿Ñ€ÐµÐ´Ð»\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: Ð¡Ð¾ÑÑ‚Ð°Ð²ÑŒ Ð´Ð¸Ð°Ð»Ð¾Ð³ (5 Ñ€ÐµÐ¿Ð»Ð¸Ðº Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñ‹) Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐŸÐ¾Ð´Ð±ÐµÑ€Ð¸ 5 ÑÐ¸Ð½Ð¾Ð½Ð¸Ð¼Ð¾Ð² ÑÐ»Ð¾Ð²Ð° â€žschnellâ€œ Ð¸ ÑÐ¾ÑÑ‚Ð°Ð²ÑŒ Ñ ÐºÐ°Ð¶Ð´Ñ‹Ð¼ ÐºÐ¾\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ñ€Ð°Ð·Ð½Ð¸Ñ†Ñƒ Ð¼ÐµÐ¶Ð´Ñƒ â€žwissenâ€œ Ð¸ â€žkennenâ€œ Ð¸ Ð´Ð°Ð¹ Ð¿Ð¾ 2 Ð¿Ñ€Ð¸Ð¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐšÐ°Ðº Ð¿Ñ€Ð¾Ð¸Ð·Ð½Ð¾ÑÐ¸Ñ‚ÑÑ Ð½ÐµÐ¼ÐµÑ†ÐºÐ°Ñ Ð±ÑƒÐºÐ²Ð° â€žÃ¤â€œ? ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸ Ð¸ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: Ð Ð°ÑÑÐºÐ°Ð¶Ð¸ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ñƒ Ð¾ Ð¿Ñ€Ð°Ð·Ð´Ð½Ð¸ÐºÐµ Oktoberfest (ÐºÑ€Ð°Ñ‚ÐºÐ¾, 3â€“4 Ð¿Ñ€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐŸÑ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð¹ Ñ‚ÐµÐºÑÑ‚: Â«Ich heiÃŸe Anna â€¦Â» (â‰ˆ100 ÑÐ»Ð¾Ð²). Ð¡Ð¿Ñ€Ð¾ÑÐ¸ 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐÐ°Ð¿Ð¸ÑˆÐ¸ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ð¿Ð¸ÑÑŒÐ¼Ð¾-Ð¸Ð·Ð²Ð¸Ð½ÐµÐ½Ð¸Ðµ (70â€“80 ÑÐ»Ð¾Ð²) ÐºÐ¾Ð»Ð»ÐµÐ³Ðµ Ð½Ð° Ð½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ ÑÑƒÑ„Ñ„Ð¸ÐºÑ â€ž-keit/-igkeitâ€œ Ð¸ Ð¾Ð±Ñ€Ð°Ð·ÑƒÐ¹ 3 ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: Ð§Ñ‚Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ â€žins Schwarze treffenâ€œ? Ð”Ð°Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ñƒ Ð¸Ð³Ñ€Ñƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð¿Ð¾Ð¼Ð½Ð¸Ñ‚ÑŒ Ð´Ð½Ð¸ Ð½ÐµÐ´ÐµÐ»Ð¸ Ð½Ð° Ð½ÐµÐ¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Artikel Ð´Ð»Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÑƒÑ€Ð¾Ð²Ð½Ñ Ð1, Ð¸Ð·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ð»: â€žIch bin gegangen ins Kino.â€œ Ð”Ð°Ð¹ ÐºÐ¾Ð½ÑÑ‚Ñ€Ñƒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: Ð’Ð¾Ð·ÑŒÐ¼Ð¸ ÑÑ‚Ñ€Ð¾Ñ‡ÐºÑƒ Ð¸Ð· Ð¿ÐµÑÐ½Ð¸ â€ž99 Luftballonsâ€œ Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ deepseek_r1_qwen8b: Q: Ð—Ð°Ð¼Ð¾Ñ‚Ð¸Ð²Ð¸Ñ€ÑƒÐ¹ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ð° Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°Ñ‚ÑŒ ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð½ÐµÐ¼ÐµÑ†ÐºÐ¸Ð¹, ÑƒÐ¿Ð¾Ð¼ÑÐ½Ð¸ 3\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: resultsTeacherDE\\deepseek_r1_qwen8b.json, resultsTeacherDE\\deepseek_r1_qwen8b.csv\n",
      "ðŸ”„ mistral: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10f614ac65e402c98df1dc711dce1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Perfekt Ð³Ð»Ð°Ð³Ð¾Ð»Ð¾Ð² Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð½Ð° \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐšÐ¾Ð³Ð´Ð° Ð² Ð½ÐµÐ¼ÐµÑ†ÐºÐ¾Ð¼ ÑƒÐ¿Ð¾Ñ‚Ñ€ÐµÐ±Ð»ÑÐµÑ‚ÑÑ Konjunktiv II? ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ð¾-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¸Ð¹ Ð¸ Ð¿Ñ€Ð¾ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹ Ð³Ñ€Ð°Ð¼Ð¼Ð°Ñ‚Ð¸ÐºÑƒ: â€žIch hab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸ Ð½Ð° Ð½ÐµÐ¼ÐµÑ†ÐºÐ¸Ð¹: Â«Ð¯ Ñ…Ð¾Ñ‚ÐµÐ» Ð±Ñ‹ Ð·Ð°ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ñ‡Ð°ÑˆÐºÑƒ ÐºÐ¾Ñ„Ðµ, Ð¿Ð¾\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚ Ð½Ð°Ð¿Ð¸ÑÐ°Ð»: â€žIch gehe am Montag ins Schule.â€œ ÐÐ°Ð¹Ð´Ð¸ Ð¾\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚: â€žEr hat ein Auto gekauft gestern.â€œ Ð˜ÑÐ¿Ñ€Ð°Ð²ÑŒ Ð¿Ñ€ÐµÐ´Ð»\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: Ð¡Ð¾ÑÑ‚Ð°Ð²ÑŒ Ð´Ð¸Ð°Ð»Ð¾Ð³ (5 Ñ€ÐµÐ¿Ð»Ð¸Ðº Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñ‹) Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐŸÐ¾Ð´Ð±ÐµÑ€Ð¸ 5 ÑÐ¸Ð½Ð¾Ð½Ð¸Ð¼Ð¾Ð² ÑÐ»Ð¾Ð²Ð° â€žschnellâ€œ Ð¸ ÑÐ¾ÑÑ‚Ð°Ð²ÑŒ Ñ ÐºÐ°Ð¶Ð´Ñ‹Ð¼ ÐºÐ¾\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ñ€Ð°Ð·Ð½Ð¸Ñ†Ñƒ Ð¼ÐµÐ¶Ð´Ñƒ â€žwissenâ€œ Ð¸ â€žkennenâ€œ Ð¸ Ð´Ð°Ð¹ Ð¿Ð¾ 2 Ð¿Ñ€Ð¸Ð¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐšÐ°Ðº Ð¿Ñ€Ð¾Ð¸Ð·Ð½Ð¾ÑÐ¸Ñ‚ÑÑ Ð½ÐµÐ¼ÐµÑ†ÐºÐ°Ñ Ð±ÑƒÐºÐ²Ð° â€žÃ¤â€œ? ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸ Ð¸ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: Ð Ð°ÑÑÐºÐ°Ð¶Ð¸ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ñƒ Ð¾ Ð¿Ñ€Ð°Ð·Ð´Ð½Ð¸ÐºÐµ Oktoberfest (ÐºÑ€Ð°Ñ‚ÐºÐ¾, 3â€“4 Ð¿Ñ€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐŸÑ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð¹ Ñ‚ÐµÐºÑÑ‚: Â«Ich heiÃŸe Anna â€¦Â» (â‰ˆ100 ÑÐ»Ð¾Ð²). Ð¡Ð¿Ñ€Ð¾ÑÐ¸ 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐÐ°Ð¿Ð¸ÑˆÐ¸ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ð¿Ð¸ÑÑŒÐ¼Ð¾-Ð¸Ð·Ð²Ð¸Ð½ÐµÐ½Ð¸Ðµ (70â€“80 ÑÐ»Ð¾Ð²) ÐºÐ¾Ð»Ð»ÐµÐ³Ðµ Ð½Ð° Ð½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ ÑÑƒÑ„Ñ„Ð¸ÐºÑ â€ž-keit/-igkeitâ€œ Ð¸ Ð¾Ð±Ñ€Ð°Ð·ÑƒÐ¹ 3 ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: Ð§Ñ‚Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ â€žins Schwarze treffenâ€œ? Ð”Ð°Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ñƒ Ð¸Ð³Ñ€Ñƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð¿Ð¾Ð¼Ð½Ð¸Ñ‚ÑŒ Ð´Ð½Ð¸ Ð½ÐµÐ´ÐµÐ»Ð¸ Ð½Ð° Ð½ÐµÐ¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Artikel Ð´Ð»Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÑƒÑ€Ð¾Ð²Ð½Ñ Ð1, Ð¸Ð·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ð»: â€žIch bin gegangen ins Kino.â€œ Ð”Ð°Ð¹ ÐºÐ¾Ð½ÑÑ‚Ñ€Ñƒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: Ð’Ð¾Ð·ÑŒÐ¼Ð¸ ÑÑ‚Ñ€Ð¾Ñ‡ÐºÑƒ Ð¸Ð· Ð¿ÐµÑÐ½Ð¸ â€ž99 Luftballonsâ€œ Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ mistral: Q: Ð—Ð°Ð¼Ð¾Ñ‚Ð¸Ð²Ð¸Ñ€ÑƒÐ¹ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ð° Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°Ñ‚ÑŒ ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð½ÐµÐ¼ÐµÑ†ÐºÐ¸Ð¹, ÑƒÐ¿Ð¾Ð¼ÑÐ½Ð¸ 3\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: resultsTeacherDE\\mistral.json, resultsTeacherDE\\mistral.csv\n",
      "ðŸ”„ Meta Llama: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c8e0d06fbe47108ff5af126660a6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Perfekt Ð³Ð»Ð°Ð³Ð¾Ð»Ð¾Ð² Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð½Ð° \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐšÐ¾Ð³Ð´Ð° Ð² Ð½ÐµÐ¼ÐµÑ†ÐºÐ¾Ð¼ ÑƒÐ¿Ð¾Ñ‚Ñ€ÐµÐ±Ð»ÑÐµÑ‚ÑÑ Konjunktiv II? ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ð¾-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¸Ð¹ Ð¸ Ð¿Ñ€Ð¾ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹ Ð³Ñ€Ð°Ð¼Ð¼Ð°Ñ‚Ð¸ÐºÑƒ: â€žIch hab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸ Ð½Ð° Ð½ÐµÐ¼ÐµÑ†ÐºÐ¸Ð¹: Â«Ð¯ Ñ…Ð¾Ñ‚ÐµÐ» Ð±Ñ‹ Ð·Ð°ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ñ‡Ð°ÑˆÐºÑƒ ÐºÐ¾Ñ„Ðµ, Ð¿Ð¾\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚ Ð½Ð°Ð¿Ð¸ÑÐ°Ð»: â€žIch gehe am Montag ins Schule.â€œ ÐÐ°Ð¹Ð´Ð¸ Ð¾\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚: â€žEr hat ein Auto gekauft gestern.â€œ Ð˜ÑÐ¿Ñ€Ð°Ð²ÑŒ Ð¿Ñ€ÐµÐ´Ð»\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: Ð¡Ð¾ÑÑ‚Ð°Ð²ÑŒ Ð´Ð¸Ð°Ð»Ð¾Ð³ (5 Ñ€ÐµÐ¿Ð»Ð¸Ðº Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñ‹) Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐŸÐ¾Ð´Ð±ÐµÑ€Ð¸ 5 ÑÐ¸Ð½Ð¾Ð½Ð¸Ð¼Ð¾Ð² ÑÐ»Ð¾Ð²Ð° â€žschnellâ€œ Ð¸ ÑÐ¾ÑÑ‚Ð°Ð²ÑŒ Ñ ÐºÐ°Ð¶Ð´Ñ‹Ð¼ ÐºÐ¾\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ñ€Ð°Ð·Ð½Ð¸Ñ†Ñƒ Ð¼ÐµÐ¶Ð´Ñƒ â€žwissenâ€œ Ð¸ â€žkennenâ€œ Ð¸ Ð´Ð°Ð¹ Ð¿Ð¾ 2 Ð¿Ñ€Ð¸Ð¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐšÐ°Ðº Ð¿Ñ€Ð¾Ð¸Ð·Ð½Ð¾ÑÐ¸Ñ‚ÑÑ Ð½ÐµÐ¼ÐµÑ†ÐºÐ°Ñ Ð±ÑƒÐºÐ²Ð° â€žÃ¤â€œ? ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸ Ð¸ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: Ð Ð°ÑÑÐºÐ°Ð¶Ð¸ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ñƒ Ð¾ Ð¿Ñ€Ð°Ð·Ð´Ð½Ð¸ÐºÐµ Oktoberfest (ÐºÑ€Ð°Ñ‚ÐºÐ¾, 3â€“4 Ð¿Ñ€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐŸÑ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð¹ Ñ‚ÐµÐºÑÑ‚: Â«Ich heiÃŸe Anna â€¦Â» (â‰ˆ100 ÑÐ»Ð¾Ð²). Ð¡Ð¿Ñ€Ð¾ÑÐ¸ 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐÐ°Ð¿Ð¸ÑˆÐ¸ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ð¿Ð¸ÑÑŒÐ¼Ð¾-Ð¸Ð·Ð²Ð¸Ð½ÐµÐ½Ð¸Ðµ (70â€“80 ÑÐ»Ð¾Ð²) ÐºÐ¾Ð»Ð»ÐµÐ³Ðµ Ð½Ð° Ð½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ ÑÑƒÑ„Ñ„Ð¸ÐºÑ â€ž-keit/-igkeitâ€œ Ð¸ Ð¾Ð±Ñ€Ð°Ð·ÑƒÐ¹ 3 ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: Ð§Ñ‚Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ â€žins Schwarze treffenâ€œ? Ð”Ð°Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ñƒ Ð¸Ð³Ñ€Ñƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð¿Ð¾Ð¼Ð½Ð¸Ñ‚ÑŒ Ð´Ð½Ð¸ Ð½ÐµÐ´ÐµÐ»Ð¸ Ð½Ð° Ð½ÐµÐ¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Artikel Ð´Ð»Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÑƒÑ€Ð¾Ð²Ð½Ñ Ð1, Ð¸Ð·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ð»: â€žIch bin gegangen ins Kino.â€œ Ð”Ð°Ð¹ ÐºÐ¾Ð½ÑÑ‚Ñ€Ñƒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: Ð’Ð¾Ð·ÑŒÐ¼Ð¸ ÑÑ‚Ñ€Ð¾Ñ‡ÐºÑƒ Ð¸Ð· Ð¿ÐµÑÐ½Ð¸ â€ž99 Luftballonsâ€œ Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Meta Llama: Q: Ð—Ð°Ð¼Ð¾Ñ‚Ð¸Ð²Ð¸Ñ€ÑƒÐ¹ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ð° Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°Ñ‚ÑŒ ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð½ÐµÐ¼ÐµÑ†ÐºÐ¸Ð¹, ÑƒÐ¿Ð¾Ð¼ÑÐ½Ð¸ 3\n",
      "âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹: resultsTeacherDE\\Meta Llama.json, resultsTeacherDE\\Meta Llama.csv\n"
     ]
    }
   ],
   "source": [
    "teacher_prompts = [\n",
    "    # Ð“Ñ€Ð°Ð¼Ð¼Ð°Ñ‚Ð¸ÐºÐ°\n",
    "    \"Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Perfekt Ð³Ð»Ð°Ð³Ð¾Ð»Ð¾Ð² Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ Ð¸ Ð¿Ñ€Ð¸Ð²ÐµÐ´Ð¸ 2 Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°.\\nA:\",\n",
    "    \"Q: ÐšÐ¾Ð³Ð´Ð° Ð² Ð½ÐµÐ¼ÐµÑ†ÐºÐ¾Ð¼ ÑƒÐ¿Ð¾Ñ‚Ñ€ÐµÐ±Ð»ÑÐµÑ‚ÑÑ Konjunktiv II? ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸ Ð¸ Ð´Ð¾Ð±Ð°Ð²ÑŒ 3 Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°.\\nA:\",\n",
    "    # ÐŸÐµÑ€ÐµÐ²Ð¾Ð´ + Ñ€Ð°Ð·Ð±Ð¾Ñ€\n",
    "    \"Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¸Ð¹ Ð¸ Ð¿Ñ€Ð¾ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹ Ð³Ñ€Ð°Ð¼Ð¼Ð°Ñ‚Ð¸ÐºÑƒ: â€žIch habe daran gedacht, dich anzurufen.â€œ\\nA:\",\n",
    "    \"Q: ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸ Ð½Ð° Ð½ÐµÐ¼ÐµÑ†ÐºÐ¸Ð¹: Â«Ð¯ Ñ…Ð¾Ñ‚ÐµÐ» Ð±Ñ‹ Ð·Ð°ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ñ‡Ð°ÑˆÐºÑƒ ÐºÐ¾Ñ„Ðµ, Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°Â» Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð²Ñ‹Ð±Ð¾Ñ€ Ñ„Ð¾Ñ€Ð¼Ñ‹ Ð³Ð»Ð°Ð³Ð¾Ð»Ð°.\\nA:\",\n",
    "    # Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾ÑˆÐ¸Ð±Ð¾Ðº ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ð°\n",
    "    \"Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚ Ð½Ð°Ð¿Ð¸ÑÐ°Ð»: â€žIch gehe am Montag ins Schule.â€œ ÐÐ°Ð¹Ð´Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÑƒ Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾.\\nA:\",\n",
    "    \"Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚: â€žEr hat ein Auto gekauft gestern.â€œ Ð˜ÑÐ¿Ñ€Ð°Ð²ÑŒ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº ÑÐ»Ð¾Ð².\\nA:\",\n",
    "    # Ð”Ð¸Ð°Ð»Ð¾Ð³\n",
    "    \"Q: Ð¡Ð¾ÑÑ‚Ð°Ð²ÑŒ Ð´Ð¸Ð°Ð»Ð¾Ð³ (5 Ñ€ÐµÐ¿Ð»Ð¸Ðº Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñ‹) Ð¼ÐµÐ¶Ð´Ñƒ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÐµÐ¼ Ð¸ ÑƒÑ‡ÐµÐ½Ð¸ÐºÐ¾Ð¼ Ð¾ Ð¿Ð¾ÐºÑƒÐ¿ÐºÐµ Ð±Ð¸Ð»ÐµÑ‚Ð° Ð½Ð° Ð¿Ð¾ÐµÐ·Ð´ Ð² Ð‘ÐµÑ€Ð»Ð¸Ð½Ðµ.\\nA:\",\n",
    "    # Ð›ÐµÐºÑÐ¸ÐºÐ°\n",
    "    \"Q: ÐŸÐ¾Ð´Ð±ÐµÑ€Ð¸ 5 ÑÐ¸Ð½Ð¾Ð½Ð¸Ð¼Ð¾Ð² ÑÐ»Ð¾Ð²Ð° â€žschnellâ€œ Ð¸ ÑÐ¾ÑÑ‚Ð°Ð²ÑŒ Ñ ÐºÐ°Ð¶Ð´Ñ‹Ð¼ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ.\\nA:\",\n",
    "    \"Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ñ€Ð°Ð·Ð½Ð¸Ñ†Ñƒ Ð¼ÐµÐ¶Ð´Ñƒ â€žwissenâ€œ Ð¸ â€žkennenâ€œ Ð¸ Ð´Ð°Ð¹ Ð¿Ð¾ 2 Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°.\\nA:\",\n",
    "    # ÐŸÑ€Ð¾Ð¸Ð·Ð½Ð¾ÑˆÐµÐ½Ð¸Ðµ/Ñ„Ð¾Ð½ÐµÑ‚Ð¸ÐºÐ°\n",
    "    \"Q: ÐšÐ°Ðº Ð¿Ñ€Ð¾Ð¸Ð·Ð½Ð¾ÑÐ¸Ñ‚ÑÑ Ð½ÐµÐ¼ÐµÑ†ÐºÐ°Ñ Ð±ÑƒÐºÐ²Ð° â€žÃ¤â€œ? ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸ Ð¸ Ð¿Ñ€Ð¸Ð²ÐµÐ´Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ ÑÐ»Ð¾Ð²Ð°.\\nA:\",\n",
    "    # ÐšÑƒÐ»ÑŒÑ‚ÑƒÑ€Ð°\n",
    "    \"Q: Ð Ð°ÑÑÐºÐ°Ð¶Ð¸ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ñƒ Ð¾ Ð¿Ñ€Ð°Ð·Ð´Ð½Ð¸ÐºÐµ Oktoberfest (ÐºÑ€Ð°Ñ‚ÐºÐ¾, 3â€“4 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ). Ð—Ð°Ñ‚ÐµÐ¼ Ð·Ð°Ð´Ð°Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾ Ñ‚ÐµÐ¼Ðµ.\\nA:\",\n",
    "    # Ð”Ð»Ð¸Ð½Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚\n",
    "    \"Q: ÐŸÑ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ð¹ Ñ‚ÐµÐºÑÑ‚: Â«Ich heiÃŸe Anna â€¦Â» (â‰ˆ100 ÑÐ»Ð¾Ð²). Ð¡Ð¿Ñ€Ð¾ÑÐ¸ 3 Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° Ð¿Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸ÑŽ Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑŒ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ.\\nA:\",\n",
    "    # ÐŸÐ¸ÑÑŒÐ¼Ð¾\n",
    "    \"Q: ÐÐ°Ð¿Ð¸ÑˆÐ¸ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ð¿Ð¸ÑÑŒÐ¼Ð¾-Ð¸Ð·Ð²Ð¸Ð½ÐµÐ½Ð¸Ðµ (70â€“80 ÑÐ»Ð¾Ð²) ÐºÐ¾Ð»Ð»ÐµÐ³Ðµ Ð½Ð° Ð½ÐµÐ¼ÐµÑ†ÐºÐ¾Ð¼ Ð¸ Ð¿Ð¾ÑÑÐ½Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¿Ð¸ÑÑŒÐ¼Ð°.\\nA:\",\n",
    "    # Ð¡Ð»Ð¾Ð²Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ\n",
    "    \"Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ ÑÑƒÑ„Ñ„Ð¸ÐºÑ â€ž-keit/-igkeitâ€œ Ð¸ Ð¾Ð±Ñ€Ð°Ð·ÑƒÐ¹ 3 ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð¾Ñ‚ Ð¿Ñ€Ð¸Ð»Ð°Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ….\\nA:\",\n",
    "    # Ð£ÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ñ‹Ðµ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ\n",
    "    \"Q: Ð§Ñ‚Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ â€žins Schwarze treffenâ€œ? Ð”Ð°Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´ Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÑ€ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ð¸.\\nA:\",\n",
    "    # Ð˜Ð³Ñ€Ð¾Ð²Ð¾Ðµ ÑƒÐ¿Ñ€Ð°Ð¶Ð½ÐµÐ½Ð¸Ðµ\n",
    "    \"Q: ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ñƒ Ð¸Ð³Ñ€Ñƒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ð¿Ð¾Ð¼Ð½Ð¸Ñ‚ÑŒ Ð´Ð½Ð¸ Ð½ÐµÐ´ÐµÐ»Ð¸ Ð½Ð° Ð½ÐµÐ¼ÐµÑ†ÐºÐ¾Ð¼.\\nA:\",\n",
    "    # ÐÐ´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ñ ÑƒÑ€Ð¾Ð²Ð½Ñ\n",
    "    \"Q: ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Artikel Ð´Ð»Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÑƒÑ€Ð¾Ð²Ð½Ñ Ð1, Ð¸Ð·Ð±ÐµÐ³Ð°Ñ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð¾Ð².\\nA:\",\n",
    "    # ÐžÐ±Ñ€Ð°Ñ‚Ð½Ð°Ñ ÑÐ²ÑÐ·ÑŒ\n",
    "    \"Q: Ð¡Ñ‚ÑƒÐ´ÐµÐ½Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ð»: â€žIch bin gegangen ins Kino.â€œ Ð”Ð°Ð¹ ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¸Ð²Ð½ÑƒÑŽ Ð¾Ð±Ñ€Ð°Ñ‚Ð½ÑƒÑŽ ÑÐ²ÑÐ·ÑŒ Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÑƒ.\\nA:\",\n",
    "    # Ð Ð°Ð·Ð±Ð¾Ñ€ Ð¿ÐµÑÐ½Ð¸\n",
    "    \"Q: Ð’Ð¾Ð·ÑŒÐ¼Ð¸ ÑÑ‚Ñ€Ð¾Ñ‡ÐºÑƒ Ð¸Ð· Ð¿ÐµÑÐ½Ð¸ â€ž99 Luftballonsâ€œ Ð¸ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ÑƒÑŽ Ð² Ð½ÐµÐ¹ Ð³Ñ€Ð°Ð¼Ð¼Ð°Ñ‚Ð¸ÐºÑƒ.\\nA:\",\n",
    "    # ÐœÐ¾Ñ‚Ð¸Ð²Ð°Ñ†Ð¸Ñ\n",
    "    \"Q: Ð—Ð°Ð¼Ð¾Ñ‚Ð¸Ð²Ð¸Ñ€ÑƒÐ¹ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ð° Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°Ñ‚ÑŒ ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð½ÐµÐ¼ÐµÑ†ÐºÐ¸Ð¹, ÑƒÐ¿Ð¾Ð¼ÑÐ½Ð¸ 3 Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ñ‹.\\nA:\",\n",
    "]\n",
    "\n",
    "models = {\n",
    "    \"deepseek_r1_qwen8b\":  \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"Meta Llama\": \"meta-llama/Meta-Llama-3-8B\"\n",
    "}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    mdl = tok = None\n",
    "    print(f\"ðŸ”„ {name}: Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽâ€¦\")\n",
    "    try:\n",
    "        tok, mdl = load_model(model_id)          # load model\n",
    "        evaluate_model(mdl, tok, name, prompts=teacher_prompts, \n",
    "                       max_new_tokens=320, do_sample=False, temperature=0.2,\n",
    "                       save_dir=\"resultsTeacherDE\")           # test model\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {name}: {e}\")\n",
    "    finally:\n",
    "        del mdl, tok                             # unload model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_time_sec</th>\n",
       "      <th>prompt_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral</td>\n",
       "      <td>15.4310</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta Llama</td>\n",
       "      <td>18.6665</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepseek_r1_qwen8b</td>\n",
       "      <td>23.1650</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  avg_time_sec  prompt_count\n",
       "2             mistral       15.4310            20\n",
       "0          Meta Llama       18.6665            20\n",
       "1  deepseek_r1_qwen8b       23.1650            20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate models\n",
    "summary_df = summarize_all_results(result_dir=\"../results/resultsTeacherDE\")\n",
    "display(summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
