{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "import torch\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.utils import login_huggingface, load_model\n",
    "from utils.main_train_model import evaluate_model, summarize_all_results\n",
    "from config import RESULTS_DIR\n",
    "\n",
    "login_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: загружаю…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356adcc386844f36a9ee2220400fcfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Кто такой Альберт Эйнштейн?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Объясни, как работает градиентный спуск простыми словами\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Придумай короткий диалог между учителем и учеником на тему экологии\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Переведи: 'I am testing a language model' и объясни перевод.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Придумай фантастическое животное и опиши, где оно живёт\n",
      "A:\n",
      "✅ Результаты сохранены: results\\mistral.json, results\\mistral.csv\n",
      "🔄 deepseek_chat: загружаю…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23680c14a8864ef5afa5d1a84c8beef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_chat: Q: Кто такой Альберт Эйнштейн?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_chat: Q: Объясни, как работает градиентный спуск простыми словами\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_chat: Q: Придумай короткий диалог между учителем и учеником на тему экологии\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_chat: Q: Переведи: 'I am testing a language model' и объясни перевод.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_chat: Q: Придумай фантастическое животное и опиши, где оно живёт\n",
      "A:\n",
      "✅ Результаты сохранены: results\\deepseek_chat.json, results\\deepseek_chat.csv\n",
      "🔄 deepseek_base: загружаю…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90999c729ea846e58c16b640113a3f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_base: Q: Кто такой Альберт Эйнштейн?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_base: Q: Объясни, как работает градиентный спуск простыми словами\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_base: Q: Придумай короткий диалог между учителем и учеником на тему экологии\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_base: Q: Переведи: 'I am testing a language model' и объясни перевод.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_base: Q: Придумай фантастическое животное и опиши, где оно живёт\n",
      "A:\n",
      "✅ Результаты сохранены: results\\deepseek_base.json, results\\deepseek_base.csv\n",
      "🔄 zephyr: загружаю…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fc8048b40146219442e25557f5482d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 zephyr: Q: Кто такой Альберт Эйнштейн?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 zephyr: Q: Объясни, как работает градиентный спуск простыми словами\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 zephyr: Q: Придумай короткий диалог между учителем и учеником на тему экологии\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 zephyr: Q: Переведи: 'I am testing a language model' и объясни перевод.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 zephyr: Q: Придумай фантастическое животное и опиши, где оно живёт\n",
      "A:\n",
      "✅ Результаты сохранены: results\\zephyr.json, results\\zephyr.csv\n",
      "🔄 Meta Llama: загружаю…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a47ce85977f488baa8d4685de978a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Кто такой Альберт Эйнштейн?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Объясни, как работает градиентный спуск простыми словами\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Придумай короткий диалог между учителем и учеником на тему экологии\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Переведи: 'I am testing a language model' и объясни перевод.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Придумай фантастическое животное и опиши, где оно живёт\n",
      "A:\n",
      "✅ Результаты сохранены: results\\Meta Llama.json, results\\Meta Llama.csv\n",
      "🔄 MS phi-2: загружаю…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee3b7e97a9e41bc8f54cccebb6bebe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 MS phi-2: Q: Кто такой Альберт Эйнштейн?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 MS phi-2: Q: Объясни, как работает градиентный спуск простыми словами\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 MS phi-2: Q: Придумай короткий диалог между учителем и учеником на тему экологии\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 MS phi-2: Q: Переведи: 'I am testing a language model' и объясни перевод.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 MS phi-2: Q: Придумай фантастическое животное и опиши, где оно живёт\n",
      "A:\n",
      "✅ Результаты сохранены: results\\MS phi-2.json, results\\MS phi-2.csv\n",
      "🔄 deepseek_r1_qwen8b: загружаю…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11cae1415a6433bb064c8b46ab49ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Кто такой Альберт Эйнштейн?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Объясни, как работает градиентный спуск простыми словами\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Придумай короткий диалог между учителем и учеником на тему экологии\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Переведи: 'I am testing a language model' и объясни перевод.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Придумай фантастическое животное и опиши, где оно живёт\n",
      "A:\n",
      "✅ Результаты сохранены: results\\deepseek_r1_qwen8b.json, results\\deepseek_r1_qwen8b.csv\n",
      "🔄 deepseek_r1_llama8b: загружаю…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efafbad5a2648f496e47c75eb36252e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_llama8b: Q: Кто такой Альберт Эйнштейн?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_llama8b: Q: Объясни, как работает градиентный спуск простыми словами\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_llama8b: Q: Придумай короткий диалог между учителем и учеником на тему экологии\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_llama8b: Q: Переведи: 'I am testing a language model' и объясни перевод.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_llama8b: Q: Придумай фантастическое животное и опиши, где оно живёт\n",
      "A:\n",
      "✅ Результаты сохранены: results\\deepseek_r1_llama8b.json, results\\deepseek_r1_llama8b.csv\n",
      "🔄 Google gemma_7b: загружаю…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52313f9170a489faa7d674648fe8d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Google gemma_7b: Q: Кто такой Альберт Эйнштейн?\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Google gemma_7b: Q: Объясни, как работает градиентный спуск простыми словами\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Google gemma_7b: Q: Придумай короткий диалог между учителем и учеником на тему экологии\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Google gemma_7b: Q: Переведи: 'I am testing a language model' и объясни перевод.\n",
      "A:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Google gemma_7b: Q: Придумай фантастическое животное и опиши, где оно живёт\n",
      "A:\n",
      "✅ Результаты сохранены: results\\Google gemma_7b.json, results\\Google gemma_7b.csv\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"deepseek_chat\":  \"deepseek-ai/deepseek-llm-7b-chat\",\n",
    "    \"deepseek_base\":  \"deepseek-ai/deepseek-llm-7b-base\",\n",
    "    \"zephyr\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"Meta Llama\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"MS phi-2\": \"microsoft/phi-2\",\n",
    "    \"deepseek_r1_qwen8b\":  \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "    \"deepseek_r1_llama8b\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"Google gemma_7b\": \"google/gemma-7b\"    \n",
    "}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    mdl = tok = None\n",
    "    print(f\"🔄 {name}: loading…\")\n",
    "    try:\n",
    "        tok, mdl = load_model(model_id)          # load model\n",
    "        evaluate_model(mdl, tok, name)           # evaluate model\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {name}: {e}\")\n",
    "    finally:\n",
    "        del mdl, tok                             # unload model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_time_sec</th>\n",
       "      <th>prompt_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MS phi-2</td>\n",
       "      <td>3.246</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek_chat</td>\n",
       "      <td>4.640</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mistral</td>\n",
       "      <td>4.920</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deepseek_base</td>\n",
       "      <td>5.260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meta Llama</td>\n",
       "      <td>5.720</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google gemma_7b</td>\n",
       "      <td>5.940</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek_r1_llama8b</td>\n",
       "      <td>6.478</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>zephyr</td>\n",
       "      <td>6.642</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deepseek_r1_qwen8b</td>\n",
       "      <td>8.062</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  avg_time_sec  prompt_count\n",
       "1             MS phi-2         3.246             5\n",
       "4        deepseek_chat         4.640             5\n",
       "7              mistral         4.920             5\n",
       "3        deepseek_base         5.260             5\n",
       "2           Meta Llama         5.720             5\n",
       "0      Google gemma_7b         5.940             5\n",
       "5  deepseek_r1_llama8b         6.478             5\n",
       "8               zephyr         6.642             5\n",
       "6   deepseek_r1_qwen8b         8.062             5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore models\n",
    "summary_df = summarize_all_results()\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: загружаю…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d59983bd35341de94d00670e3fc4c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Объясни правило образования Perfekt глаголов движения на \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Когда в немецком употребляется Konjunktiv II? Объясни по-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Переведи на русский и прокомментируй грамматику: „Ich hab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Переведи на немецкий: «Я хотел бы заказать чашку кофе, по\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Студент написал: „Ich gehe am Montag ins Schule.“ Найди о\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Студент: „Er hat ein Auto gekauft gestern.“ Исправь предл\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Составь диалог (5 реплик с каждой стороны) между учителем\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Подбери 5 синонимов слова „schnell“ и составь с каждым ко\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Объясни разницу между „wissen“ и „kennen“ и дай по 2 прим\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Как произносится немецкая буква „ä“? Объясни по-русски и \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Расскажи студенту о празднике Oktoberfest (кратко, 3–4 пр\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Прочитай текст: «Ich heiße Anna …» (≈100 слов). Спроси 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Напиши краткое письмо-извинение (70–80 слов) коллеге на н\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Объясни суффикс „-keit/-igkeit“ и образуй 3 существительн\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Что значит выражение „ins Schwarze treffen“? Дай перевод \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Предложи студенту игру, чтобы запомнить дни недели на нем\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Объясни правило Artikel для существительных уровня А1, из\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Студент ответил: „Ich bin gegangen ins Kino.“ Дай констру\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Возьми строчку из песни „99 Luftballons“ и объясни исполь\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 deepseek_r1_qwen8b: Q: Замотивируй студента продолжать учить немецкий, упомяни 3\n",
      "✅ Результаты сохранены: resultsTeacherDE\\deepseek_r1_qwen8b.json, resultsTeacherDE\\deepseek_r1_qwen8b.csv\n",
      "🔄 mistral: загружаю…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10f614ac65e402c98df1dc711dce1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Объясни правило образования Perfekt глаголов движения на \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Когда в немецком употребляется Konjunktiv II? Объясни по-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Переведи на русский и прокомментируй грамматику: „Ich hab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Переведи на немецкий: «Я хотел бы заказать чашку кофе, по\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Студент написал: „Ich gehe am Montag ins Schule.“ Найди о\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Студент: „Er hat ein Auto gekauft gestern.“ Исправь предл\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Составь диалог (5 реплик с каждой стороны) между учителем\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Подбери 5 синонимов слова „schnell“ и составь с каждым ко\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Объясни разницу между „wissen“ и „kennen“ и дай по 2 прим\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Как произносится немецкая буква „ä“? Объясни по-русски и \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Расскажи студенту о празднике Oktoberfest (кратко, 3–4 пр\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Прочитай текст: «Ich heiße Anna …» (≈100 слов). Спроси 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Напиши краткое письмо-извинение (70–80 слов) коллеге на н\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Объясни суффикс „-keit/-igkeit“ и образуй 3 существительн\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Что значит выражение „ins Schwarze treffen“? Дай перевод \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Предложи студенту игру, чтобы запомнить дни недели на нем\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Объясни правило Artikel для существительных уровня А1, из\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Студент ответил: „Ich bin gegangen ins Kino.“ Дай констру\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Возьми строчку из песни „99 Luftballons“ и объясни исполь\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 mistral: Q: Замотивируй студента продолжать учить немецкий, упомяни 3\n",
      "✅ Результаты сохранены: resultsTeacherDE\\mistral.json, resultsTeacherDE\\mistral.csv\n",
      "🔄 Meta Llama: загружаю…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c8e0d06fbe47108ff5af126660a6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Объясни правило образования Perfekt глаголов движения на \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Когда в немецком употребляется Konjunktiv II? Объясни по-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Переведи на русский и прокомментируй грамматику: „Ich hab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Переведи на немецкий: «Я хотел бы заказать чашку кофе, по\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Студент написал: „Ich gehe am Montag ins Schule.“ Найди о\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Студент: „Er hat ein Auto gekauft gestern.“ Исправь предл\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Составь диалог (5 реплик с каждой стороны) между учителем\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Подбери 5 синонимов слова „schnell“ и составь с каждым ко\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Объясни разницу между „wissen“ и „kennen“ и дай по 2 прим\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Как произносится немецкая буква „ä“? Объясни по-русски и \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Расскажи студенту о празднике Oktoberfest (кратко, 3–4 пр\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Прочитай текст: «Ich heiße Anna …» (≈100 слов). Спроси 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Напиши краткое письмо-извинение (70–80 слов) коллеге на н\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Объясни суффикс „-keit/-igkeit“ и образуй 3 существительн\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Что значит выражение „ins Schwarze treffen“? Дай перевод \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Предложи студенту игру, чтобы запомнить дни недели на нем\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Объясни правило Artikel для существительных уровня А1, из\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Студент ответил: „Ich bin gegangen ins Kino.“ Дай констру\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Возьми строчку из песни „99 Luftballons“ и объясни исполь\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Meta Llama: Q: Замотивируй студента продолжать учить немецкий, упомяни 3\n",
      "✅ Результаты сохранены: resultsTeacherDE\\Meta Llama.json, resultsTeacherDE\\Meta Llama.csv\n"
     ]
    }
   ],
   "source": [
    "teacher_prompts = [\n",
    "    # Грамматика\n",
    "    \"Q: Объясни правило образования Perfekt глаголов движения на русском и приведи 2 примера.\\nA:\",\n",
    "    \"Q: Когда в немецком употребляется Konjunktiv II? Объясни по-русски и добавь 3 примера.\\nA:\",\n",
    "    # Перевод + разбор\n",
    "    \"Q: Переведи на русский и прокомментируй грамматику: „Ich habe daran gedacht, dich anzurufen.“\\nA:\",\n",
    "    \"Q: Переведи на немецкий: «Я хотел бы заказать чашку кофе, пожалуйста» и объясни выбор формы глагола.\\nA:\",\n",
    "    # Исправление ошибок студента\n",
    "    \"Q: Студент написал: „Ich gehe am Montag ins Schule.“ Найди ошибку и объясни корректно.\\nA:\",\n",
    "    \"Q: Студент: „Er hat ein Auto gekauft gestern.“ Исправь предложение и объясни порядок слов.\\nA:\",\n",
    "    # Диалог\n",
    "    \"Q: Составь диалог (5 реплик с каждой стороны) между учителем и учеником о покупке билета на поезд в Берлине.\\nA:\",\n",
    "    # Лексика\n",
    "    \"Q: Подбери 5 синонимов слова „schnell“ и составь с каждым короткое предложение.\\nA:\",\n",
    "    \"Q: Объясни разницу между „wissen“ и „kennen“ и дай по 2 примера.\\nA:\",\n",
    "    # Произношение/фонетика\n",
    "    \"Q: Как произносится немецкая буква „ä“? Объясни по-русски и приведи пример слова.\\nA:\",\n",
    "    # Культура\n",
    "    \"Q: Расскажи студенту о празднике Oktoberfest (кратко, 3–4 предложения). Затем задай вопрос по теме.\\nA:\",\n",
    "    # Длинный контекст\n",
    "    \"Q: Прочитай текст: «Ich heiße Anna …» (≈100 слов). Спроси 3 вопроса по содержанию и проверь понимание.\\nA:\",\n",
    "    # Письмо\n",
    "    \"Q: Напиши краткое письмо-извинение (70–80 слов) коллеге на немецком и поясни структуру письма.\\nA:\",\n",
    "    # Словообразование\n",
    "    \"Q: Объясни суффикс „-keit/-igkeit“ и образуй 3 существительных от прилагательных.\\nA:\",\n",
    "    # Устойчивые выражения\n",
    "    \"Q: Что значит выражение „ins Schwarze treffen“? Дай перевод и пример ситуации.\\nA:\",\n",
    "    # Игровое упражнение\n",
    "    \"Q: Предложи студенту игру, чтобы запомнить дни недели на немецком.\\nA:\",\n",
    "    # Адаптация уровня\n",
    "    \"Q: Объясни правило Artikel для существительных уровня А1, избегая сложных терминов.\\nA:\",\n",
    "    # Обратная связь\n",
    "    \"Q: Студент ответил: „Ich bin gegangen ins Kino.“ Дай конструктивную обратную связь и объясни ошибку.\\nA:\",\n",
    "    # Разбор песни\n",
    "    \"Q: Возьми строчку из песни „99 Luftballons“ и объясни используемую в ней грамматику.\\nA:\",\n",
    "    # Мотивация\n",
    "    \"Q: Замотивируй студента продолжать учить немецкий, упомяни 3 причины.\\nA:\",\n",
    "]\n",
    "\n",
    "models = {\n",
    "    \"deepseek_r1_qwen8b\":  \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"Meta Llama\": \"meta-llama/Meta-Llama-3-8B\"\n",
    "}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    mdl = tok = None\n",
    "    print(f\"🔄 {name}: загружаю…\")\n",
    "    try:\n",
    "        tok, mdl = load_model(model_id)          # load model\n",
    "        evaluate_model(mdl, tok, name, prompts=teacher_prompts, \n",
    "                       max_new_tokens=320, do_sample=False, temperature=0.2,\n",
    "                       save_dir=\"resultsTeacherDE\")           # test model\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {name}: {e}\")\n",
    "    finally:\n",
    "        del mdl, tok                             # unload model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_time_sec</th>\n",
       "      <th>prompt_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral</td>\n",
       "      <td>15.4310</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta Llama</td>\n",
       "      <td>18.6665</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepseek_r1_qwen8b</td>\n",
       "      <td>23.1650</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  avg_time_sec  prompt_count\n",
       "2             mistral       15.4310            20\n",
       "0          Meta Llama       18.6665            20\n",
       "1  deepseek_r1_qwen8b       23.1650            20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate models\n",
    "summary_df = summarize_all_results(result_dir=\"../results/resultsTeacherDE\")\n",
    "display(summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
